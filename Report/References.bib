% This file was created with Citavi 6.10.0.0

@proceedings{.2001,
 year = {2001}
}


@proceedings{.2019,
 year = {2019},
 title = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}},
 address = {Minneapolis, Minnesota},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{.2021,
 year = {2021},
 title = {{COLINS}}
}


@inproceedings{Blei.2001,
 author = {Blei, David and Ng, Andrew and Jordan, Michael},
 title = {{Latent Dirichlet Allocation}},
 pages = {601--608},
 volume = {3},
 year = {2001}
}


@inproceedings{Devlin.2019,
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
 url = {https://aclanthology.org/N19-1423},
 pages = {4171--4186},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}},
 year = {2019},
 address = {Minneapolis, Minnesota},
 doi = {10.18653/v1/N19-1423}
}


@inproceedings{K.Smelyakov.2021,
 author = {{K. Smelyakov} and {A. Chupryna} and {Dmytro Darahan} and {Serhii Midina}},
 title = {{Effectiveness of Modern Text Recognition Solutions and Tools for Common Data Sources}},
 booktitle = {{COLINS}},
 year = {2021}
}


@article{Li.2012,
 author = {Li, Youguo and Wu, Haiyan},
 year = {2012},
 title = {{A Clustering Method Based on K-Means Algorithm}},
 pages = {1104--1109},
 volume = {25},
 journal = {{Physics Procedia}},
 doi = {10.1016/j.phpro.2012.03.206}
}


@misc{Riba.05.10.2019,
 abstract = {This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.},
 author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
 date = {05.10.2019},
 title = {{Kornia: an Open Source Differentiable Computer Vision Library for  PyTorch}},
 url = {https://arxiv.org/pdf/1910.02190},
 file = {Riba, Mishkin et al. 05.10.2019 - Kornia an Open Source Differentiable:Attachments/Riba, Mishkin et al. 05.10.2019 - Kornia an Open Source Differentiable.pdf:application/pdf}
}


@article{Tan.,
 abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.  Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.  With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3{\%} top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0{\%} accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
 author = {Tan, Mingxing and {Le V}, Quoc},
 year = {2019},
 title = {{EfficientNetV2: Smaller Models and Faster Training}},
 url = {https://arxiv.org/pdf/2104.00298},
 journal = {{International Conference on Machine Learning}},
 file = {Tan, Le V - EfficientNetV2:Attachments/Tan, Le V - EfficientNetV2.pdf:application/pdf}
}


@misc{Zhuang.07.11.2019,
 abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
 author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
 date = {07.11.2019},
 title = {{A Comprehensive Survey on Transfer Learning}},
 url = {https://arxiv.org/pdf/1911.02685},
 file = {Zhuang, Qi et al. 07.11.2019 - A Comprehensive Survey on Transfer:Attachments/Zhuang, Qi et al. 07.11.2019 - A Comprehensive Survey on Transfer.pdf:application/pdf}
}


